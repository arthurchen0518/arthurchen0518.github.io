<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>DyVA360: Dynamic Visual-Audio Dataset for Immersive Neural Fields</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href=""> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="./DirectionNet/bootstrap.min.css">
    <link rel="stylesheet" href="./DirectionNet/font-awesome.min.css">
    <link rel="stylesheet" href="./DirectionNet/codemirror.min.css">
    <link rel="stylesheet" href="./DirectionNet/app.css">

    <link rel="stylesheet" href="./DirectionNet/bootstrap.min(1).css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-E0ZMW34H4P"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-E0ZMW34H4P');
    </script>
    
    <script src="./DirectionNet/jquery.min.js"></script>
    <script src="./DirectionNet/bootstrap.min.js"></script>
    <script src="./DirectionNet/codemirror.min.js"></script>
    <script src="./DirectionNet/clipboard.min.js"></script>

    <script src="./DirectionNet/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                DyVA360: Dynamic Visual-Audio Dataset for Immersive Neural Fields<br>
                <small>
<!--                     NeurIPS 2022 -->
                </small>
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://johnnylu305.github.io/">
                          Cheng-You Lu
                        </a>
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/peisen-zhou-a11332135/">
                          Peisen Zhou
                        </a>
                    </li>
                    <li>
                        <a href="https://xing-angela.github.io/">
                          Angela Xing
                        </a>
                    </li>
                    <li>
                        <a href="https://coreqode.github.io/">
                          Chandradeep Pokhariya
                        </a>
                    </li>
                    <li>
                        <a href="https://www.arnabdey.co/">
                          Arnab Dey
                        </a>
                    </li>
                    <li>
                         Ishaan Nikhil Shah
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/rugvedm/">
                          Rugved Mavidipalli
                        </a>
                    </li>
                    <li>
                        <a href="https://dylan.hu/">
                          Dylan Hu
                        </a>
                    </li>
                    <li>
                        <a href="https://www.i3s.unice.fr/robotvision/index.php/the-team-2/andrew-comport">
                          Andrew I. Comport
                        </a>
                    </li>
                    <li>
                        <a href="https://arthurchen0518.github.io/">
                          Kefan Chen
                        </a>
                    </li>
                    <li>
                        <a href="https://cs.brown.edu/people/ssrinath/">
                          Srinath Sridhar
                        </a>
                    </li>
                </ul>
                Brown University &nbsp&nbsp IIIT Hyderabad &nbsp&nbsp I3S-CNRS/Université Côte d’Azur  <br>
            </div>
        </div>


<!-- 
        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2106.03336">
                            <img src="./DirectionNet/paper.PNG" height="120px"><br>
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/tJ2JTFmi0oI">
                            <img src="./DirectionNet/youtube_icon_dark.png" height="120px"><br>
                                <h4><strong>Technical Video </strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/arthurchen0518/DirectionNet">
                            <img src="./DirectionNet/github_pad.png" height="120px"><br>
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
                <br>
        </div> -->


<!--         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Example Real Results: View Synthesis, Relighting and Material editing
                </h3>
                <video id="v0" width="100%" autoplay="" loop="" muted="" controls="">
                  <source src="files/real-world_results.mp4" type="video/mp4">
                </video>
            </div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
<!--                 <img src="./DirectionNet/TrainingPipeline.png" class="img-responsive" alt="overview"><br> -->
                <h3>
                    Abstract
                </h3>
                
                <p class="text-justify">
            Recent advance in neural radiance field (NeRF) has achieved high-fidelity photorealistic synthesis for images and dynamic videos from novel views, paving the way for immersive experiences in visual content. Nevertheless, the representation of motion and sound, which are integral components of real-world multimedia experiences, often falls short in current NeRF research, which primarily focuses on static scenes or brief dynamic sequences lasting only a few seconds.
The gap between existing research and practical application stems largely from the absence of extensive long-sequence multiview datasets and benchmarks.
To address this, we present the first large-scale omniview visual-audio dataset of real-world dynamic scenes to facilitate research in tackling various NeRF challenges. Our dataset captures dynamic human activities and objects in motion, reflecting the complexity and diversity of daily life. Alongside this rich dynamic content, our dataset also incorporates canonicalized static objects and is comprehensively annotated with captions for every scene. We present our system design to capture the audio and 360$^{\circ}$ view images from 53 synchronized high-resolution cameras at 120 fps.</p>
            </div>
        </div>


<!--         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Technical Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="./files/...html" allowfullscreen="" style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->

<!--         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly="" style="display: none;">
@InProceedings{Chen_2021_CVPR,
    author    = {Chen, Kefan and Snavely, Noah and Makadia, Ameesh},
    title     = {Wide-Baseline Relative Camera Pose Estimation With Directional Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {3258-3268}
}
                    </textarea>
                </div>
            </div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                <p></p>
            </div>
        </div>
    <!-- </div> -->


</body></html>
